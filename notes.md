# Master Class Structure
1. Introduction: Transformers in 15 minutes
 + Generative AI: Generating output, e.g. sequence2sequence
 + Transformer Models: A type of neural architecture
 + Picture of a transformer model
 + Encoder-Decoder
 + Transformer block
2. Some simple generation exercises
3. Below the generation:
 + Probability distribution
 + Temperature?
5. From the other end: Input
 + Tokenization
6. Semantic spaces:
 + The inside of the model
 + Attention across embeddings
7. From beginning to end
8. Training

# Topics

## Preprocessing
- Subtokenization
-

## Semantic Spaces
- Plot general
- Same word in different contexts
  + ambiguity
- Concreteness?

## Architecture
- Attention
- BERT vs. GPT-2
- Embeddings across layers

## Training
- The tiniest Transformer

## Generation
- Probability distribution of an MLM
- Fail cases, provide examples
  + Negation
  + Irrelevant changes
